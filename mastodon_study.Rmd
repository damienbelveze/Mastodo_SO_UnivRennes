---
title: "Bilan d'une année d'expérience avec Mastodon"
subtitle: "activité du compte Mastodon du service ARDEL"
author: "Damien Belvèze" 
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: false
bibliography: biblio.bib
csl: nature.csl
date: "2025-01-15"
---


```{r 1_authentification avec rtoot, eval=FALSE, include=FALSE}
library(rtoot)
auth_setup()
# Si ces deux lignes de code ne sont pas exécutées en amont de la compilation (knit) de ce document, une erreur 503 est affichée. 
# Pour exécuter ce chunck, il faut avoir son navigateur et avoir ouvert le compte SO_UnivRennes et donc pour cela disposer de l'identifiant et du mot de passe du compte
```

```{r extraction de données avec rtoot, include=FALSE}
library(readr)
library(rtoot)
library(tidyverse)
library(dplyr)

id <- "112370075539544475"
followers <- get_account_followers(id, limit = 300L)
following <- get_account_following(id, limit = 300L)
lists <- get_account_lists(id, token = NULL, parse = TRUE)
print(lists)
#relationships <- get_account_relationships(id, limit = 300L)
dataframe <- get_account_statuses(id, limit = 400L)
toots <- get_account_statuses(id, exclude_reblogs = TRUE, limit = 400L)
sent <- as.integer(length(dataframe$id))
toots <- as.integer(length(toots$id))
boosts <- (sent - toots)
reblogs <- sum(dataframe$reblogs_count)
favourites <- sum(dataframe$favourites_count)
replies <- sum(dataframe$replies_count)
reply_to <-dataframe$in_reply_to_id

followers_number <- length(followers$id)
following_number <- length(following$id)
#relationships_number <- length(relationships$id)
status_number <- length(dataframe$id)
#print(dataframe)

write_csv(dataframe, "SO_univrennes_toots.csv")
write_csv(followers, "followers.csv")
#write_csv(relationships, "relationships.csv")
#print(followers)
#print(followers_number)
```


```{r replies to users, include=FALSE}

reply_to <- dataframe$in_reply_to_account_id %>%
    .[!is.na(.) & . != "NA"]   #
count_reply_to <- length(reply_to)

```

# Pourquoi nous avons choisi Mastodon

## Périmètre du compte

ARDEL, auquel est rattaché le [compte SO_UnivRennes](https://mastodon.social/@SO_UnivRennes) est le service d'"Appui à la Recherche et à la Documentation En Ligne (ARDEL) du Service Commun de Documentation (SCD) de l'Université de Rennes. L'activité d'ARDEL ne se limite pas à la Science Ouverte, mais elle y prend néanmoins une part importante dans les fiches de postes de 4 agents sur les 6 que compte le service, exclusive chez 2 d'entre eux. 

Le compte SO_UnivRennes ne parle d'ailleurs pas uniquement de Science Ouverte mais a vocation à partager des informations sur l'ensemble des activités des membres du groupe ARDEL, dans lesquelles on trouve la documentation en ligne gérée par le SCD, les formations doctorales et la gestion des thèses en tant que production scientifique de notre Université. Pour le reste, en effet, la Science Ouverte recouvre la sensibilisation et l'accompagnement des chercheurs et chercheuses (y compris les doctorant.e.s) à la conservation, à la diffusion et au partage des publications dans et depuis HAL, des données dans et depuis Recherche Data Gouv ou d'autres entrepôts de confiance et du code source dans Software Heritage et depuis HAL.

Inversement, au sein de l'Université de Rennes, la Science Ouverte est présente bien au delà de la cellule ARDEL et infuse les politiques de l'Université. Elle dispose d'un chargé de mission qui est aussi chargé de la documentation. 
Ces éléments qui ne figurent pas encore sur la "bio" de notre compte a du être précisée dans un message de réponse à une utilisatrice, enseignante-chercheuse à l'Université de Rennes, qui sollicitait notre avis sur une contradiction apparente entre la théorie et la pratique de la Science Ouverte par notre Université.
Le Service Commun de la Documentation a vocation à contribuer par son expertise à la construction de la politique Science Ouverte de l'Université, mais celle-ci est en dernière instance définie par le conseil scientifique. Le compte SO_UnivRennes communique sur les éléments de cette politique et aide les chercheurs et les chercheuses inscrit.e.s sur Mastodon à la mettre en oeuvre au quotidien.  

## Pourquoi Mastodon

Un service comme ARDEL pourrait se satisfaire de l'usage de son site web pour communiquer et de l'utilisation de flux RSS pour suivre les principales évolutions sur la Science Ouverte et la documentation électronique tant les sources d'information fiables et inspirantes sont nombreuses :  sites de revues spécialisées, sites d'institutions de recherche, de chercheurs et chercheuses, et de personnes appartenant au monde de la documentation et des bibliothèques. Devant le discrédit dans lequel X plonge une partie des autres réseaux sociaux, beaucoup d'individus actifs dans le monde du logiciel ou de la recherche quittent ces plateformes dites sociales pour passer plus de temps à lire les billets des sites qu'elles apprécient et à écrire et publier des billets sur leur propre site. 

### Les flux RSS et les sites web ne suffisent pas

Pour notre part, à titre personnel, mais aussi en tant que service, nous considérons que la fréquentation d'un réseau social ajoute à une veille qui reposerait sur des flux l'apport de conversations entre points de vue parfois opposés sur les objets de notre activité ; être témoins de ces conversations, nous permet de prendre en considération dans notre pratique davantage de points de vue et d'affiner nos réponses en fonction de la situation de nos interlocuteurs et interlocutrices. Prendre part à ces conversations permet a fortiori d'éprouver des arguments au contact de collègues ou de personnels de recherche qui ont une autre expérience de terrain que la nôtre. 

Nous pensons toutefois que ces enrichissements ne peuvent avoir lieu que si leur contexte est parfaitement transparent. 
Par transparence, nous entendons tout d'abord celle de l'algorithme de distribution des message : 
Tout d'abord, nous devons nous assurer que tout ce qui est posté par un.e utilisatreur.ice du réseau nous parvient sans filtre, à la manière d'un flux RSS qui reproduit fidèlement tout ce qui est posté par la personne qu'on suit. Ce n'était plus le cas sur Twitter déjà bien avant qu'Elon Musk ne le rachète. Mastodon n'est qu'un logiciel dont le code source est accessible. Le réseau social Fedivers dépend d'un protocole (ActivityPub) qui préexistait au logiciel Mastodon et son code est également en accès libre. 

### Communiquer, c'est faire du Commun

Nous souhaitons par ailleurs investir du temps dans un Commun, car nous considérons que communiquer, ce n'est pas seulement échanger des messages, mais aussi contribuer au milieu (medium) qui permettra à ces échanges d'avoir lieu. Communiquer, comme le rappelle Arthur Perret, c'est aussi bâtir du Commun (@perretFaireCommun2023a), a contrario des réseaux sociaux centralisés qui privatisent (de la donnée personnelle) et privilégient des points de vue au détriment d'autres par des algorithmes opaques et déloyaux. 
Créer du commun, c'est précisément interroger le medium qu'on utilise, ses règles, ses principes de modération, son articulation avec les médias voisins quand elle est techniquement possible. Par exemple, les instances de Mastodon peuvent techniquement communiquer avec celles de Threads (Meta), mais ne le souhaitent pas pour éviter que le Fédivers ne soit victime de la part de Meta d'une manoeuvre de style **embrace, extend and extinguish**. Avec Mastodon, il n'y a pas de propriétaire qui décide de cette interopérabilité pour tous. L'administrateurice d'une instance peut rendre le choix de l'ouverture possible à ses utilisateurices (sans pouvoir la rendre obligatoire) ou bien proposer que la connexion ne soit pas possible pour la raison stratégique évoquée. Ces débats ont lieu au sein des instances avec les utilisateurices. Il n'y pas de procédure de vote à notre connaissance, mais les personnes mécontentes de la décision prise peuvent sans perdre l'accès à leur communauté de followers quitter cette instance pour une autre qui correspond mieux à leurs options d'ouverture (ou de fermeture). Cet exemple, montre qu'il est possible de bâtir des Communs avec Mastodon, alors qu'un gouvernement centralisé dans les mains d'un propriétaire qui dispose de toutes les règles et décide de tous les paramétrages obère d'emblée cette construction.  

### La décentralisation, l'atout maître de Mastodon

Répétons-le, Mastodon n'est qu'un logiciel et pas un réseau social qui permet d'accéder à un réseau structuré par un protocole d'échange, ActivityPub, l'un et l'autre sont libres. 
Pour les utilisateurices de X qui ne supportent pas la dégradation de plus en plus nette de leur expérience sur ce réseau et la toxicité accrue du réseau du fait d'une absence de modération, la difficulté d'intégrer Mastodon est en réalité ce qui constitue la richesse de cet écosystème : Mastodon comporte un grand nombre d'instances de tailles et de politiques différentes, et il faut en choisir une : Où atterrir ?
Nous nous sommes évidemment posé la question et nous avons comparé les diverses instances qui accueillent les acteurs de la Science ainsi que celles qui travaillent dans le périmètre de l'Etat. Nous avons d'abord contacté l'instance gérée par la Direction du Numérique (Dinum) qui nous paraissait la plus appropriée, mais faute de réponse de l'administrateur à ce moment-là, nous avons opté pour la solution généraliste *mastodon.social*, celle qui compte le plus grand nombre de comptes hébergés à ce jour. Depuis, lors nous avons échangé avec les administrateurs de l'instance *[social.numerique.gouv.fr](https://social.numerique.gouv.fr/explore)* de la DINUM qui nous incitent à choisir leur serveur dédié aux services de l'Etat. Dans le même temps, nous sommes en contact avec un agent du Service interuniversitaire en charge de la mutualisation numérique en île-de-France (UNIF). L'UNIF se propose de lancer prochainement une instance destinée au monde universitaire qui intègrerait le protocole Shibboleth pour permettre à ses membres d'accéder à leur compte via leurs identifiants universitaires. Cette possibilité rendrait notre compte plus résistant aux attaques d'usurpation, dans la mesure où sur une instance dépourvue de ce type de gestion de droits, il n'est pas possible de mettre en place un système de double authentification impliquant une application idoine sur un smartphone commun. 

Nous avons mentionné la dégradation de l'expérience usager sur une plateforme comme X/Twitter. Cory Doctorow qui a étudié ce phénomène commun à nombre de plateformes ultra-dominantes sur le marché à forgé à ce sujet le concept d'emmerdification (enshittification). Ce phénomène décrit la manière dont les propriétaires d'une plateforme accordent une telle importance au retour sur investissement pour satisfaire la demande actionnariale (ou politique en ce qui concerne Meta et Twitter) que non seulement les demandes des usagers ne sont plus prises en compte, mais en plus celles des partenaires commerciaux de moindre ampleur (publicitaires) sont également négligées. Les usagers, pour ne parler que d'eux, subissent cette dégradation pour ne pas perdre leur communauté et leurs contacts en quittant la plateforme et cela jusqu'au point de rupture qui peut prendre la forme d'un harcèlement. 
aussi pour Cory Doctorow, ce n'est pas tant le fait que des capitaux privés soutiennent une infrastructure qui condamnent celle-ci à l'emmerdification à court ou moyen terme, mais le fait qu'elle rend ses usagers captifs de cette infrastructure (@doctorowEnshittificationIsntCaused2024). A contrario, comme on l'a vu plus haut, le Fédivers conserve à ses utilisateurices toute latitude pour quitter une instance dont l'administration ou la politique générale leur déplairait pour une autre plus proche de leurs options, et cela sans aucune perte de leur expérience passée. Comptes suivis, followers, listes, messages envoyés accompagnent le ou la migrante vers sa nouvelle maison.
Cory Doctorow observe que si cette liberté est inscrite dans le code du protocole de Bluesky (AT), elle n'est toujours pas effective et rien ne garantit qu'elle ne le soit un jour (@doctorowBlueskyEnshittification2025) et que cette incertitude devrait nous conduire à opter pour la solution qui tient d'emblée ses engagements (Mastodon).
Sur la récente capitalisation de Bluesky, Joan Westenberg a un point de vue divergent de celui de Doctorow. Pointant du doigt l'arrivée massive sur Bluesky d'utilisateurices fuyant Twitter, elle pose la question suivante : Est-ce qu'une plateforme financée à coups de le grand capital (elle était évaluée à 700 millions de dollars en décembre 2024) peut réellement servir de refuge à des gens qui fuient l'arbitraire et les obessions des *Tech Bros* de Meta et X ? Le financement de Bluesky condamne ses administrateurices à la pression d'un retour sur investissement rapide, qui se traduit dans un premier temps par l'ouverture à la publicité et devrait ensuite prendre les apparences de ce que nous avons déjà observé sur Twitter (@westenbergWhatBlueskys700m2025). Ajoutons à cela que des ponts techniques (*bridges*) existent entre les protocoles de Bluesky et de Mastodon permettant aux utilisateurices de chaque plateforme de suivre celles de l'autre, ces trois raisons (décentralisation effective, organisation à but non lucratif, interopérabilité avec le protocole AT) fait qu'ARDEL n'envisage pas de créer un compte sur Bluesky et de crossposter. 

### Qu'en est-il des autres alternatives : LinkedIn et Threads

Threads partage avec Mastodon un même protocole, mais les règles y sont très différentes et la modération reste l'apanage d'une très petite équipe. Il s'agit d'un réseau centralisé, qui plus est au mains d'un propriétaire qui a donné des marques ostensibles d'allégeance au pouvoir nouvellement investi à Washington lequel est connu pour ses positions climato-dénialistes et antiscience. Threads expose du contenu provenant de comptes qu'on ne suit pas et il est impossible pour l'usager qu'il en soit autrement, on ne peut donc parler d'un algorithme loyal ni transparent. 

LinkedIn est un réseau largement utilisé par des chercheurs et les chercheuses pour entretenir un réseau professionnel afin d'y trouver une visibilité pour de futurs employeurs et des opportunités de partenariats ou d'embauches. Ces enjeux n'ont que peu de liens avec l'objet qui occupe ARDEL, la Science Ouverte. 
LinkedIn est également un réseau qui illustre le concept de capitalisme de plateforme tel que défini par Shoshana Zuboff. Les données personnelles qui y sont échangées (et vraisemblablement certaines autres qui sont directement prélevées auprès de ou de la nouvelle inscrite) servent à accroître la valeur monétaire du réseau.
La position de LinkedIn vis à vis des contenus générés par des outils d'intelligence artificielle nous semble encore plus problématique. LinkedIn a annoncé récemment que loin de réguler le contenu généré par des IA, (les bots doivent être signalés comme tels sur Mastodon), le réseau facilitait le recours à ces outils en son sein (au titre d'"assistants de rédaction"). Des études récentes estiment à 54% le contenu en anglais présent sur la plateforme qui a été généré avec l'aide d'une IA, et à 23% le contenu entièrement généré par un outil d'IA (@knibbsYesThatVirala). 
La Science Ouverte n'a rien à faire ni rien à gagner dans un univers aussi synthétique. 


# Analyse de notre activité sur Mastodon

L'activité sur Mastodon peut être mesurée et interrogée au moyens d'API. Des collectifs ont commencé à distribuer des librairies pour R et pour Python qui permettent à des chercheurs et chercheuses d'extraire des contenus de la plateforme (listes d'utilisateurs, listes de statuts comportant un mot clé donné sur une instance ou plusieurs). Mais ces librairies sont également utiles pour des administrateurices d'instances afin de connaître le contenu qui transite par leur serveur en interaction avec les autres serveurs fédérés ou aux détenteurs de comptes individuels ou collectifs comme le nôtre. 
Nous avons décidé de mener cette analyse avec un package réalisé pour R, car le mode d'authentification promu par ce package nous a semblé plus simple que celui mis en oeuvre par la [librairie Python](https://mastodonpy.readthedocs.io/en/stable/) conçue pour Mastodon. 

## Le package Rtoot

David Schoch et Chung-Hong Chan sont à l'origine du package Rtoot conçu pour R qu'ils ont présenté dans les colonnes de la revue *Mobile Media & Communication* (@schochSoftwarePresentationRtoot2023a). Ce paquet est téléchargeable [depuis le répertoire du CRAN](https://cran.r-project.org/web/packages/rtoot/index.html). Comme une partie des requêtes possibles par API nécessite une authentification, ce package gère cette authentification au moyen d'un token qui peut être obtenu quand on est connecté au compte à analyser. 

Les commandes qui déclenchent ce processus d'authentification sont indiquées ci-dessous : 

```{r eval=FALSE}
library(rtoot)
auth_setup()
```

Dans la console, l'application demande le nom de l'instance qui héberge le compte. Il faut fournir le nom de cette instance entre guillemets ( "mastodon.social" )
Puis choisir "User" si on veut obtenir les chiffres relatifs à un compte d'utilisateur. 
Si le navigateur est ouvert et que le compte est ouvert sur l'un de ses onglets, en quelques secondes une fenêtre apparaît demandant l'autorisation d'utiliser Rtoot avec le compte en question. Une fois autorisée, cette transaction aboutit à l'affichage d'un jeton (token) sous la forme d'une chaîne de caractères. Du côté de R, un popup demande ce jeton. Lorsque il lui est fourni, il est possible d'exécuter les commandes de Rtoot qui permettent d'obtenir des informations sur l'activité de ce compte. 





## Présence de Mastodon dans l'enseignement supérieur français

### Comptes gérés par des universités et instituts de recherche
```{r requête Wikidata tous pays, include=FALSE}
library(WikidataR)
# enseignants-chercheurs ayant pour employés l'Université de Rennes (Q726595) ou l'Université de Rennes 1 (Q1987282)
wikidata_df <- query_wikidata('SELECT DISTINCT ?institution ?institutionLabel ?Mastodon ?Bluesky ?LinkedIn ?Facebook ?Threads ?Instagram ?countryLabel WHERE {
  ?institution wdt:P31/wdt:P279* wd:Q31855 ;
    wdt:P17 ?country .

  {
    ?institution wdt:P4033 ?Mastodon
  } UNION {
    ?institution wdt:P12361 ?Bluesky
  } UNION {
    ?institution wdt:P4264 ?LinkedIn
  }  UNION {
    ?institution wdt:P11892 ?Threads
  } 
 #   UNION {
 #   ?institution wdt:P2003 ?Instagram
 # } UNION {
 #   ?institution wdt:P2013 ?Facebook
 # }
  
 # pour ajouter Instagram et Facebook décommenter les lignes 14 à 18 ; attention, cela peut excéder le temps défini par défaut d\'une requête (time out) et la requête peut échouer

  SERVICE wikibase:label { bd:serviceParam wikibase:language "[AUTO_LANGUAGE],en". }
}')
write.csv(wikidata_df, "wikidata_all.csv")

```

Une requête Wikidata permet d'obtenir la liste des institutions de recherche qui d'après les données de Wikidata se sont dotées d'un compte Mastodon. 
Le résultat de cette requête est lisible dans le [fichier Wikidata_all.csv](wikidata_all.csv) 



# Chiffres-clé de l'activité du compte

- Le compte Mastodon SO_UnivRennes comporte **`r followers_number` followers**. 
- Nous sommes actuellement abonnés à **`r following_number` comptes**.  
- A travers ce compte, nous avons envoyé **`r status_number` statuts** (toots) constitués à la fois de **`r toots` messages que nous avons rédigés** et de **`r boosts` messages envoyés par d'autres que nous avons repostés** (boosts)
- Les toots de SO_UnivRennes ont été **`r favourites` fois mis en favoris** par des membres du réseau  
- Ils ont reçu **`r replies` réponses** de la part d'internautes. 
- de notre côté, nous avons envoyé **`r count_reply_to` messages de réponse** à des utilisateurs de Mastodon ou de Bluesky.
- ils ont été boostés **`r reblogs` fois**. 



# followers

Qui sont nos followers et qu'est-ce qui les intéresse ? 

## présence dans des listes




## Liste des followers


## mots-clé liés aux followers


```{r echo=FALSE}
library(dplyr)
library(stringr)
#pattern <- "\"https.*/tags/.*\""
pattern <- "(?<=/tags/)[^<\"]+"
# Apply the regex and extract the matched parts into a new column 'tag'
followers <- followers %>%
  mutate(tag = str_extract(followers$note, pattern))
followers$tag <- URLdecode(followers$tag)
tag_list <- followers$tag %>%
    .[!is.na(.) & . != "NA"]   # Extract the 'tag' column as a vector
#  unique()                 # Get unique tags

# Print the list of unique tags
cat(tag_list)
write.csv(tag_list,"tag_list.csv")
write.csv(followers,"followers.csv")



```

## Instances des followers

```{r instance des followers, echo=FALSE}
# extracting instances from accounts can also be performed within rtoot : https://schochastics.github.io/rtoot2022/#/interlude-extract-instances-from-statuses-1
library(stringr)
followers_url <- (followers$url)
followers_url <- data.frame(followers_url, stringsAsFactors = FALSE)
followers_url <- followers_url %>%
  mutate(
    user_instance = str_split(followers_url, "://|@", simplify = TRUE)[, 2], # Second part (domain)
    user_alias = paste0("@", str_split(followers_url, "://|@", simplify = TRUE)[, 3]) # Third part (alias)
  )
instance <- unlist(followers_url$user_instance)
instance_count <- table(instance)
instance_count_dataframe <- as.data.frame(instance_count)
colnames(instance_count_dataframe) <- c("instance", "nombre")
instance_count_dataframe <- instance_count_dataframe %>%
  arrange(desc(nombre))
head_instance_count_dataframe <- head(instance_count_dataframe, n=12)
print(head_instance_count_dataframe)

```



```{r traitement de la colonne content, include=FALSE}
# Load necessary libraries
library(rvest)
library(purrr)

# Assuming dataframe2 is your dataframe and 'content' is the column with HTML text
dataframe$content <- map_chr(dataframe$content, function(x) {
  tryCatch({
    # Read the HTML content as text and extract plain text
    read_html(x) %>% html_text()
  }, error = function(e) {
    # Return the original content if there is an error
    return(x)
  })
})

write_csv(dataframe, "SO_univrennes_toots_content.csv")# Now dataframe2$content will have the text without HTML tags
```


## acitvité des followers


```{r audience des followers, include=FALSE}

library(rtoot)

# Account ID of the main account
id <- "112370075539544475"

# Fetch the followers list
followers_list <- get_account_followers(id)

# Check structure of followers_list
if (!is.null(followers_list) && "id" %in% names(followers_list)) {
  # Ensure display_name exists and has the correct length
  if (!"display_name" %in% names(followers_list)) {
    followers_list$display_name <- rep("", length(followers_list$id))
  }

  # Function to count followers for each user with error handling
  count_followers <- function(follower_id) {
    user_info <- tryCatch(
      get_account(follower_id), # Use get_account for specific user details
      error = function(e) {
        warning(paste("Error fetching data for user ID:", follower_id))
        return(NULL)
      }
    )
    if (!is.null(user_info) && "followers_count" %in% names(user_info)) {
      return(user_info$followers_count)
    } else {
      return(NA)
    }
  }

  # Iterate over followers and count their followers
  follower_counts <- sapply(followers_list$id, count_followers)

  # Ensure lengths match before creating data frame
  if (length(follower_counts) == length(followers_list$id)) {
    # Create a data frame to display follower counts
    result <- data.frame(
      follower_name = followers_list$display_name,
      follower_count = follower_counts,
      stringsAsFactors = FALSE
    )

    # View the results
    # result <- result[order(-result$follower_count),]
    #print(result)
    write.csv(result, "followers_count.csv")
  } else {
    stop("Mismatch in lengths between followers_list and follower_counts.")
  }
} else {
  stop("Failed to fetch followers list or unexpected structure.")
}


```

Liste des 10 followers les plus influents (ayant le plus de followers)

```{r ordonnancement activité followers, echo=FALSE}
result2 <-read.csv("followers_count.csv")
result2 <- result2[order(-result$follower_count),]
head_result2 <- head(result2, n=10)
print(head_result2)

```

# Caractéristiques des toots envoyés par SO_UnivRennes

## longueur moyenne des toots


```{r distribution longueur toots, echo=FALSE}
library(tidyverse)
library(dplyr)
extract_application <- function(application) {
  if (length(application) == 0) { 
    return(NA) 
  } else { return(application$name) }
}
```


```{r eval=FALSE, include=FALSE}
library(tidyverse)
library(ggplot2)
dataframe <- get_account_statuses(id, limit = 300L) 
tree <- dataframe %>% mutate(interface = map_chr(application, extract_application), length = nchar(content)) %>% filter(!is.na(interface)) %>% ggplot(aes(x = interface, y = length)) + geom_dotplot(binaxis = "y", stackdir = "center", dotsize = 0.5)

png("tree.png")
print(tree)
dev.off()
```








![](tree.png)

```{r eval=FALSE, include=FALSE}
dataframe %>%
  mutate(
    interface = map_chr(application, extract_application),
    length = nchar(content)
  ) %>%
  filter(!is.na(interface)) %>%
  ggplot(aes(x = interface, y = length)) +
  geom_dotplot(binaxis = "y", stackdir = "center", dotsize = 0.5)
```


```{r longueur moyenne toots, eval=FALSE, include=FALSE}
toot_length <- dataframe%>%summarise(mean(length(application)))
print(toot_length)
```

Les toots de SO_UnivRennes ont en moyenne une longueur de **292 caractères**

# Liste des hashtags utilisés 

```{r liste hashtags, echo=FALSE}
library(stringr)

# Extrait tous les hashtags de la colonne "content" du fichier statuts (SO_UNivRennes_toots.csv) au moyen d'une expression régulière
dataframe$hashtags <- str_extract_all(dataframe$content, "#\\w+")
# constitue une liste avec l'ensemble des hashtags présents dans ce tableau
hashtags <- unlist(dataframe$hashtags)

# compte la fréquence de tous ces hashtags
hashtag_count <- table(hashtags)

# convertit le résultat de l'opération antérieure sous la forme d'un dataframe
hashtag_count_dataframe <- as.data.frame(hashtag_count)

# renomme les colonnes du tableau
colnames(hashtag_count_dataframe) <- c("hashtag", "nombre")

# ordonne la liste des hashtage par ordre décroissant de fréquence
hashtag_count_dataframe <- hashtag_count_dataframe %>%
  arrange(desc(nombre))

# affiche le résultat de ce traitemeent
head_hashtag_count <- head(hashtag_count_dataframe, n=10)

print(head_hashtag_count)
```

# langue utilisée pour les toots de SO_UnivRennes

```{r langue utilisée, include=FALSE}
library(textcat)

# le package textcat permet de distinguer les contenus selon la langue
dataframe$language <- textcat(dataframe$content)
print(dataframe$language)
# fait figurer les valeurs relatives à chaque langue dans un tableau
language_count <- table(dataframe$language)


# compte le nombre de textes en français et en anglais
language_count <- dataframe %>%
  filter(!is.na(language) & language != "") %>%
  group_by(language) %>%
  summarise(count = n(), .groups = "drop")

english_count <- language_count %>%
  filter(language == "english") %>%
  pull(count)

french_count <- language_count %>%
  filter(language == "french") %>%
  pull(count)


language_pie <- data.frame(
  language = c("English", "French"),
  count = c(english_count, french_count)
)
```

```{r graphique langues, echo=FALSE}
# Create the pie chart
ggplot(language_pie, aes(x = "", y = count, fill = language)) +
  geom_col(width = 1) + # Bar chart
  coord_polar(theta = "y") + # Convert to pie chart
  labs(title = "Language Distribution", x = NULL, y = NULL) +
  theme_void() + # Remove unnecessary chart elements
  theme(legend.title = element_blank())
```

## Annexes

### Messages envoyés à des internautes

Avec qui avons-nous engagé la conversation ?

```{r messages envoyés en réppnse, echo=FALSE}
usernames <- sapply(reply_to, function(id) {
  account <- get_account(id)  # Fetch account details for each ID
  return(account$acct)        # Extract and return the username
})

# Print the list of usernames
print(usernames)

```


### Timeline

```{r timeline du compte, eval=FALSE, include=FALSE}
library(rtoot)
# voir https://gesistsa.github.io/rtoot/reference/get_timeline_home.html
list_id <- c(id)
timeline <- get_timeline_list(
  list_id,
  #max_id,
  #since_id,
  #min_id,
  limit = 200L,
  token = "mettre le token ici",
  parse = TRUE,
  retryonratelimit = TRUE,
  verbose = TRUE
)
```


# Références



