---
title: "Bilan d'une année d'expérience avec Mastodon"
author: "Damien Belvèze" 
output: html_document
date: "2025-01-15"
---


```{r authentification avec rtoot, eval=FALSE, include=FALSE}
library(rtoot)
auth_setup()
```

```{r extraction de données avec rtoot, eval=FALSE, include=FALSE}
library(readr)
id <- "112370075539544475"
followers <- get_account_followers(id, limit = 200L)
relationships <- get_account_relationships(id)
dataframe <- get_account_statuses(id, limit = 300L)
print(followers)
write_csv(dataframe, "SO_univrennes_toots.csv")
write_csv(followers, "followers.csv")
write_csv(relationships, "relationships.csv")
```



```{r load statuses, include=FALSE}
library(dplyr)
library(tidyverse)
dataframe <- read.csv('SO_univrennes_toots_saved.csv')
followers <- read.csv('followers.csv')
print(dataframe)
```

# followers

## Liste des followers

```{r followers list, echo=FALSE}
followers_url <- (followers$url)
print(followers$url)

```

## Instances des followers

```{r instance des followers, echo=FALSE}
# extracting instances from accounts can also be performed within rtoot : https://schochastics.github.io/rtoot2022/#/interlude-extract-instances-from-statuses-1
library(stringr)
followers_url <- data.frame(followers_url, stringsAsFactors = FALSE)
followers_url <- followers_url %>%
  mutate(
    user_instance = str_split(followers_url, "://|@", simplify = TRUE)[, 2], # Second part (domain)
    user_alias = paste0("@", str_split(followers_url, "://|@", simplify = TRUE)[, 3]) # Third part (alias)
  )
instance <- unlist(followers_url$user_instance)
instance_count <- table(instance)
instance_count_dataframe <- as.data.frame(instance_count)
colnames(instance_count_dataframe) <- c("instance", "nombre")
instance_count_dataframe <- instance_count_dataframe %>%
  arrange(desc(nombre))
print(instance_count_dataframe)

```


```{r traitement de la colonne content, include=FALSE}
# Load necessary libraries
library(rvest)
library(purrr)

# Assuming dataframe2 is your dataframe and 'content' is the column with HTML text
dataframe$content <- map_chr(dataframe$content, function(x) {
  tryCatch({
    # Read the HTML content as text and extract plain text
    read_html(x) %>% html_text()
  }, error = function(e) {
    # Return the original content if there is an error
    return(x)
  })
})

write_csv(dataframe, "SO_univrennes_toots_content.csv")# Now dataframe2$content will have the text without HTML tags
```


## acitvité des followers


```{r}
library(rtoot)

# Account ID of the main account
id <- "112370075539544475"

# Fetch the followers list
followers_list <- get_account_followers(id)

# Check structure of followers_list
if (!is.null(followers_list) && "id" %in% names(followers_list)) {
  # Function to count followers for each user with error handling
  count_followers <- function(follower_id) {
    user_info <- tryCatch(
      get_account(follower_id), # Use get_account for specific user details
      error = function(e) {
        warning(paste("Error fetching data for user ID:", follower_id))
        return(NULL)
      }
    )
    if (!is.null(user_info) && "followers_count" %in% names(user_info)) {
      return(user_info$followers_count)
    } else {
      return(NA)
    }
  }

  # Iterate over followers and count their followers
  follower_counts <- sapply(followers_list$id, count_followers)

  # Create a data frame to display follower counts
  result <- data.frame(
    follower_name = followers_list$display_name,
    follower_count = follower_counts,
    stringsAsFactors = FALSE,
  )

  # View the results
  #result <- result[order(-result$follower_count),]
  print(result)
  write.csv(result, "followers_count.csv")
} else {
  stop("Failed to fetch followers list or unexpected structure.")
}


```

```{r}
result2 <-read.csv("followers_count.csv")
result2 <- result2[order(-result$follower_count),]
print(result2)

```

## relationships


## longueur des toots

### répartition des toots par leur longueur en nombre de caractères

```{r distribution longueur toots, echo=FALSE}
library(tidyverse)
library(dplyr)
extract_application <- function(application) {
  if (length(application) == 0) { 
    return(NA) 
  } else { return(application$name) }
}
```


```{r eval=FALSE, include=FALSE}
library(tidyverse)
library(ggplot2)
dataframe <- get_account_statuses(id, limit = 300L) 
tree <- dataframe %>% mutate(interface = map_chr(application, extract_application), length = nchar(content)) %>% filter(!is.na(interface)) %>% ggplot(aes(x = interface, y = length)) + geom_dotplot(binaxis = "y", stackdir = "center", dotsize = 0.5)

png("tree.png")
print(tree)
dev.off()
```








![](tree.png)

```{r eval=FALSE, include=FALSE}
dataframe %>%
  mutate(
    interface = map_chr(application, extract_application),
    length = nchar(content)
  ) %>%
  filter(!is.na(interface)) %>%
  ggplot(aes(x = interface, y = length)) +
  geom_dotplot(binaxis = "y", stackdir = "center", dotsize = 0.5)
```


### Longueur moyenne des toots en nombre de caractères : 

```{r longueur moyenne toots, echo=FALSE}
toot_length <- dataframe%>%summarise(mean(length(application)))
print(toot_length)
```


# Liste des hashtags utilisés 

```{r liste hashtags, echo=FALSE}
library(stringr)

# Extract all words starting with '#' from the 'content' column
dataframe$hashtags <- str_extract_all(dataframe$content, "#\\w+")
# Unnest the list of hashtags into a single vector
hashtags <- unlist(dataframe$hashtags)

# Count the frequency of each hashtag
hashtag_count <- table(hashtags)

# Convert the result into a data frame (optional, for easier handling)
hashtag_count_dataframe <- as.data.frame(hashtag_count)

# Rename columns for clarity
colnames(hashtag_count_dataframe) <- c("hashtag", "count")

# Sort the data frame by frequency (optional)
hashtag_count_dataframe <- hashtag_count_dataframe %>%
  arrange(desc(count))

# View the resulting data frame
print(hashtag_count_dataframe)
```

# langue utilisée pour les toots de SO_UnivRennes

```{r langue utilisée, include=FALSE}
library(textcat)
dataframe$language <- textcat(dataframe$content)
print(dataframe$language)
language_count <- table(dataframe$language)
library(dplyr)

# Count the number of texts in each language (English vs French)
language_count <- dataframe %>%
  filter(!is.na(language) & language != "") %>%
  group_by(language) %>%
  summarise(count = n(), .groups = "drop")

english_count <- language_count %>%
  filter(language == "english") %>%
  pull(count)

french_count <- language_count %>%
  filter(language == "french") %>%
  pull(count)

# Print the results
print(english_count)
print(french_count)


#ggplot(language_count, aes(x = "", y = count, fill = language)) +
#  geom_col(width = 1) +
#  coord_polar(theta = "y") +
#  labs(title = "Distribution des Langues dans les toots", x = NULL, y = NULL) +
#  theme_void() +
#  theme(legend.title = element_blank())
```
```{r echo=FALSE}

language_pie <- data.frame(
  language = c("English", "French"),
  count = c(english_count, french_count)
)

# Create the pie chart
ggplot(language_pie, aes(x = "", y = count, fill = language)) +
  geom_col(width = 1) + # Bar chart
  coord_polar(theta = "y") + # Convert to pie chart
  labs(title = "Language Distribution", x = NULL, y = NULL) +
  theme_void() + # Remove unnecessary chart elements
  theme(legend.title = element_blank())
```

## Annexes

### Timeline

```{r timeline du compte, eval=FALSE, include=FALSE}
library(rtoot)
# voir https://gesistsa.github.io/rtoot/reference/get_timeline_home.html
list_id <- c(id)
timeline <- get_timeline_list(
  list_id,
  #max_id,
  #since_id,
  #min_id,
  limit = 200L,
  token = "mettre le token ici",
  parse = TRUE,
  retryonratelimit = TRUE,
  verbose = TRUE
)
```


### Mastodon Python API

from mastodon import Mastodon

```{python}
from mastodon import Mastodon
Mastodon.create_app(
    'pytooterapp',
    api_base_url = 'https://mastodon.social',
    to_file = 'pytooter_clientcred.secret'
)


mastodon = Mastodon(client_id = 'pytooter_clientcred.secret',)
mastodon.log_in(
    '@SO_UnivRennes@mastodon.social',
    'tEa.-XO2wfo#',
    to_file = 'pytooter_usercred.secret'
)
```

