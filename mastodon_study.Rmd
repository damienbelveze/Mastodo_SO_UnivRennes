---
title: "Bilan d'une année d'expérience avec Mastodon"
subtitle: "activité du compte Mastodon du service ARDEL"
author: "Damien Belvèze" 
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: false
bibliography: biblio.bib
csl: nature.csl
date: "2025-01-15"
---


```{r 1_authentification avec rtoot, eval=FALSE, include=FALSE}
library(rtoot)
auth_setup()
# Si ces deux lignes de code ne sont pas exécutées en amont de la compilation (knit) de ce document, une erreur 503 est affichée. 
# Pour exécuter ce chunck, il faut avoir son navigateur et avoir ouvert le compte SO_UnivRennes et donc pour cela disposer de l'identifiant et du mot de passe du compte
```

```{r extraction de données avec rtoot, include=FALSE}
library(readr)
library(rtoot)
library(tidyverse)
library(dplyr)

id <- "112370075539544475"
followers <- get_account_followers(id, limit = 300L)
following <- get_account_following(id, limit = 300L)
lists <- get_account_lists(id, token = NULL, parse = TRUE)
print(lists)
#relationships <- get_account_relationships(id, limit = 300L)
dataframe <- get_account_statuses(id, limit = 400L)
toots <- get_account_statuses(id, exclude_reblogs = TRUE, limit = 400L)
sent <- as.integer(length(dataframe$id))
toots <- as.integer(length(toots$id))
boosts <- (sent - toots)
reblogs <- sum(dataframe$reblogs_count)
favourites <- sum(dataframe$favourites_count)
replies <- sum(dataframe$replies_count)
reply_to <-dataframe$in_reply_to_id

followers_number <- length(followers$id)
following_number <- length(following$id)
#relationships_number <- length(relationships$id)
status_number <- length(dataframe$id)
#print(dataframe)

write_csv(dataframe, "SO_univrennes_toots.csv")
write_csv(followers, "followers.csv")
#write_csv(relationships, "relationships.csv")
#print(followers)
#print(followers_number)
```


```{r replies to users, include=FALSE}

reply_to <- dataframe$in_reply_to_account_id %>%
    .[!is.na(.) & . != "NA"]   #
count_reply_to <- length(reply_to)

```

# Pourquoi nous avons choisi Mastodon

## Périmètre du compte

ARDEL, auquel est rattaché le [compte SO_UnivRennes](https://mastodon.social/@SO_UnivRennes) est le service d'"Appui à la Recherche et à la Documentation En Ligne (ARDEL) du Service Commun de Documentation (SCD) de l'Université de Rennes. L'activité d'ARDEL ne se limite pas à la Science Ouverte, mais elle y prend néanmoins une part importante dans les fiches de postes de 4 agents sur les 6 que compte le service, exclusive chez 2 d'entre eux. 

Le compte SO_UnivRennes ne parle d'ailleurs pas uniquement de Science Ouverte mais a vocation à partager des informations sur l'ensemble des activités des membres du groupe ARDEL, dans lesquelles on trouve la documentation en ligne gérée par le SCD, les formations doctorales et la gestion des thèses en tant que production scientifique de notre Université. Pour le reste, en effet, la Science Ouverte recouvre la sensibilisation et l'accompagnement des chercheurs et chercheuses (y compris les doctorant.e.s) à la conservation, à la diffusion et au partage des publications dans et depuis HAL, des données dans et depuis Recherche Data Gouv ou d'autres entrepôts de confiance et du code source dans Software Heritage et depuis HAL.

Inversement, au sein de l'Université de Rennes, la Science Ouverte est présente bien au delà de la cellule ARDEL et infuse les politiques de l'Université. Elle dispose d'un chargé de mission qui est aussi chargé de la documentation. 
Ces éléments qui ne figurent pas encore sur la "bio" de notre compte a du être précisée dans un message de réponse à une utilisatrice, enseignante-chercheuse à l'Université de Rennes, qui sollicitait notre avis sur une contradiction apparente entre la théorie et la pratique de la Science Ouverte par notre Université.
Le Service Commun de la Documentation a vocation à contribuer par son expertise à la construction de la politique Science Ouverte de l'Université, mais celle-ci est en dernière instance définie par le conseil scientifique. Le compte SO_UnivRennes communique sur les éléments de cette politique et aide les chercheurs et les chercheuses inscrit.e.s sur Mastodon à la mettre en oeuvre au quotidien.  

## Pourquoi Mastodon

Un service comme ARDEL pourrait se satisfaire de l'usage de son site web pour communiquer et de l'utilisation de flux RSS pour suivre les principales évolutions sur la Science Ouverte et la documentation électronique tant les sources d'information fiables et inspirantes sont nombreuses :  sites de revues spécialisées, sites d'institutions de recherche, de chercheurs et chercheuses, et de personnes appartenant au monde de la documentation et des bibliothèques. Devant le discrédit dans lequel X plonge une partie des autres réseaux sociaux, beaucoup d'individus actifs dans le monde du logiciel ou de la recherche quittent ces plateformes dites sociales pour passer plus de temps à lire les billets des sites qu'elles apprécient et à écrire et publier des billets sur leur propre site. 
Pour notre part, à titre personnel, mais aussi en tant que service, nous considérons que la fréquentation d'un réseau social ajoute à une veille qui reposerait sur des flux l'apport de conversations entre points de vue parfois opposés sur les objets de notre activité ; être témoins de ces conversations, nous permet de prendre en considération dans notre pratique davantage de points de vue et d'affiner nos réponses en fonction de la situation de nos interlocuteurs et interlocutrices. Prendre part à ces conversations permet a fortiori d'éprouver des arguments au contact de collègues ou de personnels de recherche qui ont une autre expérience de terrain que la nôtre. 

Nous pensons toutefois que ces enrichissements ne peuvent avoir lieu que si leur contexte est parfaitement transparent. 
Par transparence, nous entendons tout d'abord celle de l'algorithme de distribution des message : 
Tout d'abord, nous devons nous assurer que tout ce qui est posté par un.e utilisatreur.ice du réseau nous parvient sans filtre, à la manière d'un flux RSS qui reproduit fidèlement tout ce qui est posté par la personne qu'on suit. Ce n'était plus le cas sur Twitter déjà bien avant qu'Elon Musk ne le rachète. Mastodon n'est qu'un logiciel dont le code source est accessible. Le réseau social Fedivers dépend d'un protocole (ActivityPub) qui préexistait au logiciel Mastodon et son code est également en accès libre. 

Nous souhaitons par ailleurs investir du temps dans un Commun, car nous considérons que communiquer, ce n'est pas seulement échanger des messages, mais aussi contribuer au milieu (medium) qui permettra à ces échanges d'avoir lieu. Communiquer, comme le rappelle Arthur Perret, c'est aussi bâtir du Commun (@perretFaireCommun2023a), a contrario des réseaux sociaux centralisés qui privatisent (de la donnée personnelle) et privilégient des points de vue au détriment d'autres par des algorithmes opaques et déloyaux. 


## Présence de Mastodon dans l'enseignement supérieur français

```{r}
library(WikidataR)
# enseignants-chercheurs ayant pour employés l'Université de Rennes (Q726595) ou l'Université de Rennes 1 (Q1987282)
wikidata_df <- query_wikidata('SELECT DISTINCT ?institution ?institutionLabel ?mastodon WHERE {
  ?institution wdt:P31/wdt:P279* wd:Q31855 ;
    wdt:P4033 ?mastodon .
  SERVICE wikibase:label { bd:serviceParam wikibase:language "[AUTO_LANGUAGE],en". }
}')
write.csv(wikidata_df, "wikidata_all.csv")

```

Une requête Wikidata permet d'obtenir la liste des institutions de recherche qui d'après les données de Wikidata se sont dotées d'un compte Mastodon. 











# Chiffres-clé de l'activité du compte

- Le compte Mastodon SO_UnivRennes comporte **`r followers_number` followers**. 
- Nous sommes actuellement abonnés à **`r following_number` comptes**.  
- A travers ce compte, nous avons envoyé **`r status_number` statuts** (toots) constitués à la fois de **`r toots` messages que nous avons rédigés** et de **`r boosts` messages envoyés par d'autres que nous avons repostés** (boosts)
- Les toots de SO_UnivRennes ont été **`r favourites` fois mis en favoris** par des membres du réseau  
- Ils ont reçu **`r replies` réponses** de la part d'internautes. 
- de notre côté, nous avons envoyé **`r count_reply_to` messages de réponse** à des utilisateurs de Mastodon ou de Bluesky.
- ils ont été boostés **`r reblogs` fois**. 



# followers

Qui sont nos followers et qu'est-ce qui les intéresse ? 

## présence dans des listes




## Liste des followers


## mots-clé liés aux followers


```{r echo=FALSE}
library(dplyr)
library(stringr)
#pattern <- "\"https.*/tags/.*\""
pattern <- "(?<=/tags/)[^<\"]+"
# Apply the regex and extract the matched parts into a new column 'tag'
followers <- followers %>%
  mutate(tag = str_extract(followers$note, pattern))
followers$tag <- URLdecode(followers$tag)
tag_list <- followers$tag %>%
    .[!is.na(.) & . != "NA"]   # Extract the 'tag' column as a vector
#  unique()                 # Get unique tags

# Print the list of unique tags
cat(tag_list)
write.csv(tag_list,"tag_list.csv")
write.csv(followers,"followers.csv")



```

## Instances des followers

```{r instance des followers, echo=FALSE}
# extracting instances from accounts can also be performed within rtoot : https://schochastics.github.io/rtoot2022/#/interlude-extract-instances-from-statuses-1
library(stringr)
followers_url <- (followers$url)
followers_url <- data.frame(followers_url, stringsAsFactors = FALSE)
followers_url <- followers_url %>%
  mutate(
    user_instance = str_split(followers_url, "://|@", simplify = TRUE)[, 2], # Second part (domain)
    user_alias = paste0("@", str_split(followers_url, "://|@", simplify = TRUE)[, 3]) # Third part (alias)
  )
instance <- unlist(followers_url$user_instance)
instance_count <- table(instance)
instance_count_dataframe <- as.data.frame(instance_count)
colnames(instance_count_dataframe) <- c("instance", "nombre")
instance_count_dataframe <- instance_count_dataframe %>%
  arrange(desc(nombre))
head_instance_count_dataframe <- head(instance_count_dataframe, n=12)
print(head_instance_count_dataframe)

```



```{r traitement de la colonne content, include=FALSE}
# Load necessary libraries
library(rvest)
library(purrr)

# Assuming dataframe2 is your dataframe and 'content' is the column with HTML text
dataframe$content <- map_chr(dataframe$content, function(x) {
  tryCatch({
    # Read the HTML content as text and extract plain text
    read_html(x) %>% html_text()
  }, error = function(e) {
    # Return the original content if there is an error
    return(x)
  })
})

write_csv(dataframe, "SO_univrennes_toots_content.csv")# Now dataframe2$content will have the text without HTML tags
```


## acitvité des followers


```{r audience des followers, include=FALSE}

library(rtoot)

# Account ID of the main account
id <- "112370075539544475"

# Fetch the followers list
followers_list <- get_account_followers(id)

# Check structure of followers_list
if (!is.null(followers_list) && "id" %in% names(followers_list)) {
  # Ensure display_name exists and has the correct length
  if (!"display_name" %in% names(followers_list)) {
    followers_list$display_name <- rep("", length(followers_list$id))
  }

  # Function to count followers for each user with error handling
  count_followers <- function(follower_id) {
    user_info <- tryCatch(
      get_account(follower_id), # Use get_account for specific user details
      error = function(e) {
        warning(paste("Error fetching data for user ID:", follower_id))
        return(NULL)
      }
    )
    if (!is.null(user_info) && "followers_count" %in% names(user_info)) {
      return(user_info$followers_count)
    } else {
      return(NA)
    }
  }

  # Iterate over followers and count their followers
  follower_counts <- sapply(followers_list$id, count_followers)

  # Ensure lengths match before creating data frame
  if (length(follower_counts) == length(followers_list$id)) {
    # Create a data frame to display follower counts
    result <- data.frame(
      follower_name = followers_list$display_name,
      follower_count = follower_counts,
      stringsAsFactors = FALSE
    )

    # View the results
    # result <- result[order(-result$follower_count),]
    #print(result)
    write.csv(result, "followers_count.csv")
  } else {
    stop("Mismatch in lengths between followers_list and follower_counts.")
  }
} else {
  stop("Failed to fetch followers list or unexpected structure.")
}


```

Liste des 10 followers les plus influents (ayant le plus de followers)

```{r ordonnancement activité followers, echo=FALSE}
result2 <-read.csv("followers_count.csv")
result2 <- result2[order(-result$follower_count),]
head_result2 <- head(result2, n=10)
print(head_result2)

```

# Caractéristiques des toots envoyés par SO_UnivRennes

## longueur moyenne des toots


```{r distribution longueur toots, echo=FALSE}
library(tidyverse)
library(dplyr)
extract_application <- function(application) {
  if (length(application) == 0) { 
    return(NA) 
  } else { return(application$name) }
}
```


```{r eval=FALSE, include=FALSE}
library(tidyverse)
library(ggplot2)
dataframe <- get_account_statuses(id, limit = 300L) 
tree <- dataframe %>% mutate(interface = map_chr(application, extract_application), length = nchar(content)) %>% filter(!is.na(interface)) %>% ggplot(aes(x = interface, y = length)) + geom_dotplot(binaxis = "y", stackdir = "center", dotsize = 0.5)

png("tree.png")
print(tree)
dev.off()
```








![](tree.png)

```{r eval=FALSE, include=FALSE}
dataframe %>%
  mutate(
    interface = map_chr(application, extract_application),
    length = nchar(content)
  ) %>%
  filter(!is.na(interface)) %>%
  ggplot(aes(x = interface, y = length)) +
  geom_dotplot(binaxis = "y", stackdir = "center", dotsize = 0.5)
```


```{r longueur moyenne toots, eval=FALSE, include=FALSE}
toot_length <- dataframe%>%summarise(mean(length(application)))
print(toot_length)
```

Les toots de SO_UnivRennes ont en moyenne une longueur de **292 caractères**

# Liste des hashtags utilisés 

```{r liste hashtags, echo=FALSE}
library(stringr)

# Extrait tous les hashtags de la colonne "content" du fichier statuts (SO_UNivRennes_toots.csv) au moyen d'une expression régulière
dataframe$hashtags <- str_extract_all(dataframe$content, "#\\w+")
# constitue une liste avec l'ensemble des hashtags présents dans ce tableau
hashtags <- unlist(dataframe$hashtags)

# compte la fréquence de tous ces hashtags
hashtag_count <- table(hashtags)

# convertit le résultat de l'opération antérieure sous la forme d'un dataframe
hashtag_count_dataframe <- as.data.frame(hashtag_count)

# renomme les colonnes du tableau
colnames(hashtag_count_dataframe) <- c("hashtag", "nombre")

# ordonne la liste des hashtage par ordre décroissant de fréquence
hashtag_count_dataframe <- hashtag_count_dataframe %>%
  arrange(desc(nombre))

# affiche le résultat de ce traitemeent
head_hashtag_count <- head(hashtag_count_dataframe, n=10)

print(head_hashtag_count)
```

# langue utilisée pour les toots de SO_UnivRennes

```{r langue utilisée, include=FALSE}
library(textcat)

# le package textcat permet de distinguer les contenus selon la langue
dataframe$language <- textcat(dataframe$content)
print(dataframe$language)
# fait figurer les valeurs relatives à chaque langue dans un tableau
language_count <- table(dataframe$language)


# compte le nombre de textes en français et en anglais
language_count <- dataframe %>%
  filter(!is.na(language) & language != "") %>%
  group_by(language) %>%
  summarise(count = n(), .groups = "drop")

english_count <- language_count %>%
  filter(language == "english") %>%
  pull(count)

french_count <- language_count %>%
  filter(language == "french") %>%
  pull(count)


language_pie <- data.frame(
  language = c("English", "French"),
  count = c(english_count, french_count)
)
```

```{r graphique langues, echo=FALSE}
# Create the pie chart
ggplot(language_pie, aes(x = "", y = count, fill = language)) +
  geom_col(width = 1) + # Bar chart
  coord_polar(theta = "y") + # Convert to pie chart
  labs(title = "Language Distribution", x = NULL, y = NULL) +
  theme_void() + # Remove unnecessary chart elements
  theme(legend.title = element_blank())
```

## Annexes

### Messages envoyés à des internautes

Avec qui avons-nous engagé la conversation ?

```{r messages envoyés en réppnse, echo=FALSE}
usernames <- sapply(reply_to, function(id) {
  account <- get_account(id)  # Fetch account details for each ID
  return(account$acct)        # Extract and return the username
})

# Print the list of usernames
print(usernames)

```


### Timeline

```{r timeline du compte, eval=FALSE, include=FALSE}
library(rtoot)
# voir https://gesistsa.github.io/rtoot/reference/get_timeline_home.html
list_id <- c(id)
timeline <- get_timeline_list(
  list_id,
  #max_id,
  #since_id,
  #min_id,
  limit = 200L,
  token = "mettre le token ici",
  parse = TRUE,
  retryonratelimit = TRUE,
  verbose = TRUE
)
```


# Références



